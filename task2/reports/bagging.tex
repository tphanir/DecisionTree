\documentclass[11pt, a4paper]{report}

% --- SMART PACKAGE LOADING ---
\usepackage{amsmath}  % For all math environments (align, equation, etc.)
\usepackage{amssymb}  % For symbols like \mathbb
\usepackage{booktabs} % For professional tables
\usepackage{caption}  % For better control over table/figure captions
\usepackage{graphicx} % If you want to include figures later

\usepackage[
    pdftitle={Theoretical Foundations of Bagging},
    colorlinks=true,
    urlcolor=blue,
    linkcolor=black
]{hyperref} % Must be the last package

% --- DOCUMENT INFORMATION ---
\title{Theoretical Foundations and Empirical Analysis of Bootstrap Aggregating (Bagging)}


% --- BEGIN DOCUMENT ---
\begin{document}

\maketitle
\tableofcontents

\chapter{Introduction: The Problem of High Variance}
\label{chap:intro}

The experiments conducted in Task 1 confirmed that while individual decision trees are powerful classifiers with low bias, their performance is often hindered by high variance. This means their final structure is highly sensitive to small perturbations in the training data, leading to instability and poor generalization on unseen samples.

\begin{itemize}
    \item \textbf{Low Bias:} Given sufficient depth, a single tree can partition the feature space to perfectly classify the training data, thus exhibiting low systematic error.
    \item \textbf{High Variance:} The greedy, hierarchical splitting process is unstable. A small change in the training set (e.g., adding or removing a few samples) can lead to a different root node split, which cascades into a completely different final tree structure.
\end{itemize}

This variance is the primary source of overfitting. The objective of Task 2 is to develop a robust optimization that mitigates this variance, independent of the specific splitting criterion (Gini, Entropy, etc.) used. Bagging, proposed by Breiman (1996), is an ensemble meta-algorithm designed for this exact purpose.

\chapter{The Bagging Algorithm}
\label{chap:algorithm}

Bagging stands for \textbf{B}ootstrap \textbf{Agg}regat\textbf{ing}. It is a general-purpose procedure for improving unstable machine learning models by creating an ensemble. The algorithm is as follows:

\begin{enumerate}
    \item Let $D = \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$ be the original training set of size $N$.
    \item \textbf{Bootstrap:} For $i = 1$ to $B$ (e.g., $B=100$):
    \begin{itemize}
        \item Create a bootstrap sample $D_i$ by drawing $N$ instances from $D$ \textbf{with replacement}.
    \end{itemize}
    \item \textbf{Aggregate:}
    \begin{itemize}
        \item Train a base decision tree learner $h_i$ (using its full depth to maintain low bias) on each bootstrap sample $D_i$.
        \item The final aggregated classifier, $h_{\text{bag}}(x)$, makes predictions by taking a majority vote of all $B$ base learners:
        \[
        h_{\text{bag}}(x) = \arg\max_{y \in \mathcal{Y}} \sum_{i=1}^{B} \mathbb{I}(h_i(x) = y)
        \]
        where $\mathbb{I}(\cdot)$ is the indicator function.
    \end{itemize}
\end{enumerate}

\chapter{Mathematical Justification: Bias-Variance Decomposition}
\label{chap:math}

To formally prove why bagging works, we analyze its effect on the \textbf{Bias-Variance Tradeoff}. For a given test point $x$, let the true underlying function be $h(x)$ and the observed noisy data be $y = h(x) + \epsilon$, where $\epsilon$ is noise with $E[\epsilon]=0$ and $\text{Var}(\epsilon)=\sigma^2_{\epsilon}$.

The expected squared error of a learned model $\hat{h}(x)$ (trained on a dataset $D$) is:
\[
E_{D, \epsilon}\left[ (y - \hat{h}(x))^2 \right] =
\underbrace{\left( E_D[\hat{h}(x)] - h(x) \right)^2}_{\textbf{Bias}^2}
+ \underbrace{E_D\left[ (\hat{h}(x) - E_D[\hat{h}(x)])^2 \right]}_{\textbf{Variance}}
+ \underbrace{\sigma^2_{\epsilon}}_{\text{Irreducible Error}}
\]

Bagging works by directly attacking the \textbf{Variance} term while leaving the \textbf{Bias} term largely unchanged.

\section{Theorem: Bagging Reduces Estimator Variance}
\label{sec:theorem}

Let us analyze the variance of the bagged estimator. For mathematical simplicity, we use the regression case where the aggregated prediction is the average of all base learners, $\hat{h}_{\text{bag}}(x) = \frac{1}{B} \sum_{i=1}^{B} h_i(x)$. The principle holds for classification by majority vote.

\subsection{Ideal (Unrealistic) Case: Independent Learners}

Assume we could train $B$ independent learners $h_i$ on $B$ completely new datasets drawn from the true data distribution. Each learner $h_i(x)$ has variance $\sigma^2$. Then:
\[
\text{Var}(\hat{h}_{\text{bag}}(x)) = \text{Var}\left(\frac{1}{B} \sum_{i=1}^{B} h_i(x)\right)
= \frac{1}{B^2} (B\sigma^2) = \frac{\sigma^2}{B}
\]
Thus, variance decreases linearly with $B$.

\subsection{Realistic Case: Correlated Learners}

In practice, bootstrap samples $D_i$ are drawn from the same dataset, so base learners are correlated.

Let $\text{Var}(h_i(x)) = \sigma^2$ and $\text{Cov}(h_i(x), h_j(x)) = \rho\sigma^2$ for $i \neq j$, where $\rho$ is the average correlation.

Then:
\begin{align*}
\text{Var}(\hat{h}_{\text{bag}}(x)) 
&= \frac{1}{B^2} \sum_{i=1}^{B} \sum_{j=1}^{B} \text{Cov}(h_i(x), h_j(x)) \\
&= \frac{1}{B^2} \left( B\sigma^2 + B(B-1)\rho\sigma^2 \right) \\
&= \frac{\sigma^2}{B} \left( 1 + (B-1)\rho \right) \\
&= \frac{1-\rho}{B}\sigma^2 + \rho\sigma^2
\end{align*}

\subsection{Conclusion of the Theorem}

As $B \to \infty$:
\[
\lim_{B \to \infty} \text{Var}(\hat{h}_{\text{bag}}(x)) = \rho\sigma^2
\]
Since $\rho < 1$ (because bootstrap samples differ), bagging strictly reduces variance:
\[
\text{Variance(Bagged)} = \rho\sigma^2 < \sigma^2 = \text{Variance(Single Tree)}
\]

Thus, bagging reduces variance without increasing bias.

\chapter{Empirical Confirmation and Analysis}
\label{chap:results}

\section{Empirical Results}

Our empirical tests across 10 datasets confirm the theory but also reveal nuances. The results show that bagging \textbf{improves accuracy in most cases}, though not universally.

\begin{table}[h!]
\centering
\caption{Sample of Accuracy Comparison (Single Tree vs. Bagged Tree)}
\label{tab:results}
\begin{tabular}{llcc}
\toprule
\textbf{Dataset} & \textbf{Criterion} & \textbf{Single Tree Acc.} & \textbf{Bagged Tree Acc.} \\
\midrule
\multicolumn{4}{l}{\textit{Case 1: Significant Improvement (High Variance Reduction)}} \\
Balance Scale & Gini Index & 0.7340 & 0.8245 \\
Balance Scale & Hellinger & 0.6915 & 0.8191 \\
Tic-Tac-Toe & Entropy & 0.7986 & 0.8681 \\
Voting Records & Gini Index & 0.9160 & 0.9695 \\
\midrule
\multicolumn{4}{l}{\textit{Case 2: No Improvement (Stable Model or Ceiling Effect)}} \\
Nursery & Gini Index & 0.8801 & 0.8801 \\
Monks-1 & All Criteria & 1.0000 & 1.0000 \\
\midrule
\multicolumn{4}{l}{\textit{Case 3: Performance Decrease (Exceptions)}} \\
Hayes-Roth & Gain Ratio & 0.6500 & 0.6000 \\
Hayes-Roth & Hellinger & 0.4000 & 0.3000 \\
Credit Approval & Gain Ratio & 0.8551 & 0.8406 \\
Voting Records & Hellinger & 0.6565 & 0.6489 \\
\bottomrule
\end{tabular}
\end{table}

\section{Analysis of Results}

\subsection{Case 1: Significant Improvement}
Datasets like \texttt{Balance Scale} and \texttt{Tic-Tac-Toe} show large accuracy gains — classic bagging behavior, where averaging cancels individual errors and stabilizes predictions.

\subsection{Case 2: No Improvement}
On the \texttt{Nursery} dataset, trees were already stable. On \texttt{Monks-1}, perfect accuracy caused a ceiling effect.

\subsection{Case 3: Performance Decrease}
Occasional decreases in accuracy (e.g., \texttt{Hayes-Roth}) arise when bias increases slightly more than variance decreases. Two reasons:
\begin{enumerate}
    \item \textbf{Gain Ratio:} Built-in regularization interacts with bagging’s variance reduction, increasing bias.
    \item \textbf{Hellinger Distance:} For small or imbalanced datasets, bootstrapping can distort class distributions, hurting learner quality.
\end{enumerate}

\section{Lemma: The Out-of-Bag (OOB) Error}

For a dataset $D$ of size $N$, the probability that a sample is \textit{not} included in a bootstrap draw is:
\[
P(z_j \notin D_i) = \left(1 - \frac{1}{N}\right)^N \to e^{-1} \approx 0.368
\]
Thus, each instance is out-of-bag (OOB) for about 36.8\% of trees.  

By aggregating predictions from only those trees that did not train on $x_j$, we obtain an unbiased estimate of the true generalization error — without needing a separate validation set or k-fold cross-validation.

\end{document}
